[toc]



![image-20200602212731465](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200602212731465.png)

# 自己与大厂之间的差距

目前掌握技术栈有spring，springboot，mybatis，dubbo，zookeeper，redis，kafka，mysql，xxl-jobs，quartz等

-   spring会用，知道一些原理，源码没看过
-   springboot仅仅是会用，了解自动配置相关的注解，源码没有读过
-   mybatis会用，也阅读过相关的一些源码（一级缓存、二级缓存等），但是不具备设计一个ORM框架的思路
-   dubbo，了解RPC服务注册发现流程，了解dubbo的分层结构，知道支持的通信协议，序列化协议，负载均衡策略和集群容错方案等
-   zookeeper，知道CAP定理，相关的一致性算法paxos，知道zk节点种类，了解zk的一些应用方案
-   redis，了解基本数据类型和底层数据结构，了解redis的主从同步，哨兵，集群模式，了解redis 渐进式reash过程，用redis做过分布式锁的方案
-   kafka，了解整体架构，对于消息重复消费，消息丢失，消息堆积的解决方案有一定了解，但是无法做到横向对比各大MQ

项目经验，CRM后台管理系统，没多少QPS，没多少并发，所以很多数据（QPS，高峰期，TPS等）都没有注意。

生产经验，排查过几次应用频繁full gc的问题，这类问题具备一定的排查经验，linux仅仅会查询日志和看top命令。

系统设计：目前负责的CRM后台管理系统业务架构和技术架构都相对简单，对于分库分表、分布式事务的设计方案都不了解

欠缺能力：

-   技术基础
    -   常见排序算法
    -   算法思想（贪心、分治、回溯、动态规划等）
    -   leetcode 算法训练（easy,medium）
-   技术广度
    -   spring clound
-   技术深度
    -   web框架源码
        -   spring
        -   mybatis
        -   springboot
-   项目经验
    -   系统性能数据（QPS、SQL性能、数据量等）
-   生产经验
    -   部署机器配置
    -   分布式事务解决方案
    -   分库分表解决方案



# 目前系统使用那种服务框架？为什么要这样技术选型？

技术选型： 目前选用Dubbo作为服务框架，Zookeeper作为服务注册发现
原因： 
1. 之前架构师是阿里出来的，对dubbo有好感
2. 目前整个部门的系统是SOA级别的，不是微服务级别的，所以没用用上spring cloud



# Dubbo底层架构原理

![image-20200606161827138](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200606161827138.png)



# SpringCloud底层架构原理

SpringCloud说白了就是那几种组件：

1.  Eureka服务注册中心，采用多级缓存架构和读写分离优化，支持高并发
2.  Zuul网关，读取Yml文件配置，根据配置来做路由
3.  Ribbon，做负载均衡
4.  Feign，基于http做远程服务调用
5.  Hystrix做服务熔断、降级和隔离，底层基于线程池去做

## Eureka服务注册中心

![image-20200608225618852](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200608225618852.png)

## Feign服务调用

在接口上面声明注解，调用接口时通过动态代理生成特定协议格式的请求

## Ribbon负载均衡

根据客户端本地的服务注册表，通过负载均衡，选出一台机子，进行服务调用

## Zuul网关服务

配置请求和服务的对应关系

# 服务注册中心选型

常见得服务注册中心：

-   Zookeeper
-   Eureka

>   还有没那么普及得Consul、Nacos



**Eureka集群部署**

​	集群中的每个Eureka实例是平等的，各个服务可以向任何一个Eureka实例进行服务注册&发现，集群中的实例接收到的信息会互相同步，**Eureka追求PA**

![image-20200609231111415](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200609231111415.png)



**Zookeeper集群部署**

​	集群中的zk实例是主从关系，只有leader接受写请求，**Zookeeper追求PC**

![image-20200609231340013](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200609231340013.png)



**时效性对比**

-   zk的时效性更好，一般是秒级
-   eureka默认配置下，时效性非常糟糕，可以达到分钟级别



**容量对比**

-   zk不合适大规模的服务注册&发现，因为zk强一致性问题，频繁写入会产生性能问题
-   eureka也不咋的，集群部署的情况下，集群中的每个实例都需要接受所有的请求



# 网关的技术选型

**网关核心功能**

-   动态路由
-   灰度发布
-   授权认证
-   性能监控
-   系统日志
-   数据缓存（不常用）
-   限流熔断



**技术选型**

-   Kong
    基于nginx实现，开箱即用
-   Zuul
    基于Java开发，功能简单，但灰度发布、限流动态路由之类的需要自己二次开发，并发能力不高
-   Nginx + Lua（OpenResty）
    高并发能力强，但是二次开发困难



# 高并发场景下怎么对网关进行优化

集群部署zuul，根据生产环境的机器配置，判断单个网关能抗住多少请求数量，然后加机器

![image-20200611081913460](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200611081913460.png)



# 大规模（上万）服务实例部署，服务注册中心如何扛得住？

>   不管是zookeeper还是eureka，都没法抗住这么大规模的服务部署，此时需要自研服务注册中心

大规模的服务部署给注册中心带来的问题：

-   大量的服务注册表相关的数据存储（海量数据）
-   高并发请求
-   高可用架构

>   海量数据存储的解决方案就是做数据分片
>
>   高并发请求就是机器横向扩容
>
>   高可用架构通过主从切换来确保

![image-20200611221228171](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200611221228171.png)

>   自研服务注册中心的框架思路

|              | zookeeper | eureka |
| :----------: | :-------: | :----: |
| 海量数据存储 |     ×     |   ×    |
|    高并发    |     ×     |   ×    |
|    高可用    |     √     |   √    |



# 目前线上的生产部署情况

-   服务注册中心的技术选型是什麽？部署多少台机器？每台机器的配置
-   服务框架技术选型是什麽？
-   网络有没有用？
-   服务之间的超时、重试、幂等性怎么做？
-   系统每天整体流量有多少？高峰期请求量有多少？



# 目前系统性能

| btr-ncrm-web         | btr-ncrm-publish     | operation-web        | operation-service    |
| -------------------- | -------------------- | -------------------- | -------------------- |
| tp94 = 500ms         | tp99 = 50ms          | tp98 = 500ms         | tp99 = 50ms          |
| tp93 = 300ms         | tp99 = 10ms          | tp92 = 300ms         | tp98 = 10ms          |
| 每天请求量6W         | 每天请求量13W        | 每天请求量30W        | 每天请求量690W       |
| 业务高峰期QPS 3.4    | 业务高峰期QPS 1.5    | 业务高峰期QPS 18     | 业务高峰期QPS 80     |
| 最多能抗多少QPS？    | 最多能抗多少QPS？    | 最多能抗多少QPS？    | 最多能抗多少QPS？    |
| 机器配置<br />4核4GB | 机器配置<br />4核4GB | 机器配置<br />4核4GB | 机器配置<br />4核4GB |



# 接口幂等性问题解决方案

1.  数据库唯一索引（合适insert操作的防重）
2.  redis幂等性防重复框架（合适update操作的防重）



# 分布式事务解决方案

-   XA方案（两阶段提交）
-   TCC方案
-   本地消息表方案
-   可靠消息最终一致性方案
-   最大努力通知方案



## XA方案（两阶段提交）

![image-20200616214250617](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200616214250617.png)

缺点：

-   一个系统跨多个库操作，这太骚了
-   严重依赖数据库，效率低，不合适并发高的场景



## TCC方案

-   try：对各个服务的资源进行锁定
-   confirm：在各个服务中执行实际的操作
-   cancel：如果任一服务出错，则回调服务中的回滚事务的代码片段





![image-20200616214058921](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200616214058921.png)

缺点：

-   事务回滚依赖于代码补偿，恶心，难以维护

>   跟钱有关系的场景都用TCC方案



## 本地消息表

执行流程

1.  应用A执行事务，记录到消息表A，向MQ发送消息
2.  应用B消费消息，执行事务，记录到消息表B，事务执行成功，则更新消息表B记录的状态以及消息表A记录的状态
3.  应用A有定时任务轮询，检查消息表A中还没有执行成功的事务，应用A根据未完成的记录，重新发送消息，供应用B消费，直到事务执行成功



![image-20200617081634130](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200617081634130.png)



缺点：

-   严重依赖数据库管理事务，并发性能、可扩展性能弱



## 可靠消息最终一致性方案

>   不再依赖消息表实现事务，通过MQ实现事务

![image-20200617081711384](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200617081711384.png)

>   通过MQ替代本地消息表，并发性能和可扩展性有比较不错，这是目前常用的方案



## 最大努力通知方案

1.  系统A本地事务执行完之后，发送消息到MQ
2.  有个专门消费 MQ 的**最大努力通知服务**，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口
3.  要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃



# 分布式事务开源框架

-   ByteTCC
-   Himly
-   Seata



# zookeeper分布式锁羊群效应

![image-20200620091216839](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200620091216839.png)

羊群效应是指，在利用zk实现分布式锁的过程中，多个客户端同时创建、监听一个节点，加锁过程是无序，且每次节点发生变更都要同时通知多个客户端，影响zk的性能

![image-20200620091446136](https://images-1255831004.cos.ap-guangzhou.myqcloud.com/online/image-20200620091446136.png)

要解决羊群效应，应该上述这种方式，每个客户端创建的是临时顺序节点

